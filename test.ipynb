{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4161)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "f = torch.sin(x)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "value = x.grad\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven\\AppData\\Local\\Temp\\ipykernel_20912\\1066877132.py:223: UserWarning: Using a target size (torch.Size([128, 128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(current_q, expected_q.detach())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -200000000\n",
      "Episode: 1, Total Reward: -200000000\n",
      "Episode: 2, Total Reward: -200000000\n",
      "Episode: 3, Total Reward: -200000000\n",
      "Episode: 4, Total Reward: -200000000\n",
      "Episode: 5, Total Reward: -200000000\n",
      "Episode: 6, Total Reward: -200000000\n",
      "Episode: 7, Total Reward: -200000000\n",
      "Episode: 8, Total Reward: -200000000\n",
      "Episode: 9, Total Reward: -200000000\n",
      "Episode: 10, Total Reward: -200000000\n",
      "Episode: 11, Total Reward: -200000000\n",
      "Episode: 12, Total Reward: -200000000\n",
      "Episode: 13, Total Reward: -200000000\n",
      "Episode: 14, Total Reward: -200000000\n",
      "Episode: 15, Total Reward: -200000000\n",
      "Episode: 16, Total Reward: -200000000\n",
      "Episode: 17, Total Reward: -200000000\n",
      "Episode: 18, Total Reward: -200000000\n",
      "Episode: 19, Total Reward: -200000000\n",
      "Episode: 20, Total Reward: -200000000\n",
      "Episode: 21, Total Reward: -200000000\n",
      "Episode: 22, Total Reward: -200000000\n",
      "Episode: 23, Total Reward: -200000000\n",
      "Episode: 24, Total Reward: -200000000\n",
      "Episode: 25, Total Reward: -200000000\n",
      "Episode: 26, Total Reward: -200000000\n",
      "Episode: 27, Total Reward: -200000000\n",
      "Episode: 28, Total Reward: -200000000\n",
      "Episode: 29, Total Reward: -200000000\n",
      "Episode: 30, Total Reward: -200000000\n",
      "Episode: 31, Total Reward: -200000000\n",
      "Episode: 32, Total Reward: -200000000\n",
      "Episode: 33, Total Reward: -200000000\n",
      "Episode: 34, Total Reward: -200000000\n",
      "Episode: 35, Total Reward: -200000000\n",
      "Episode: 36, Total Reward: -200000000\n",
      "Episode: 37, Total Reward: -200000000\n",
      "Episode: 38, Total Reward: -200000000\n",
      "Episode: 39, Total Reward: -200000000\n",
      "Episode: 40, Total Reward: -200000000\n",
      "Episode: 41, Total Reward: -200000000\n",
      "Episode: 42, Total Reward: -200000000\n",
      "Episode: 43, Total Reward: -200000000\n",
      "Episode: 44, Total Reward: -200000000\n",
      "Episode: 45, Total Reward: -200000000\n",
      "Episode: 46, Total Reward: -200000000\n",
      "Episode: 47, Total Reward: -200000000\n",
      "Episode: 48, Total Reward: -200000000\n",
      "Episode: 49, Total Reward: -200000000\n",
      "Episode: 50, Total Reward: -200000000\n",
      "Episode: 51, Total Reward: -200000000\n",
      "Episode: 52, Total Reward: -200000000\n",
      "Episode: 53, Total Reward: -200000000\n",
      "Episode: 54, Total Reward: -200000000\n",
      "Episode: 55, Total Reward: -200000000\n",
      "Episode: 56, Total Reward: -200000000\n",
      "Episode: 57, Total Reward: -200000000\n",
      "Episode: 58, Total Reward: -200000000\n",
      "Episode: 59, Total Reward: -200000000\n",
      "Episode: 60, Total Reward: -200000000\n",
      "Episode: 61, Total Reward: -200000000\n",
      "Episode: 62, Total Reward: -200000000\n",
      "Episode: 63, Total Reward: -200000000\n",
      "Episode: 64, Total Reward: -200000000\n",
      "Episode: 65, Total Reward: -200000000\n",
      "Episode: 66, Total Reward: -200000000\n",
      "Episode: 67, Total Reward: -200000000\n",
      "Episode: 68, Total Reward: -200000000\n",
      "Episode: 69, Total Reward: -200000000\n",
      "Episode: 70, Total Reward: -200000000\n",
      "Episode: 71, Total Reward: -200000000\n",
      "Episode: 72, Total Reward: -200000000\n",
      "Episode: 73, Total Reward: -200000000\n",
      "Episode: 74, Total Reward: -200000000\n",
      "Episode: 75, Total Reward: -200000000\n",
      "Episode: 76, Total Reward: -200000000\n",
      "Episode: 77, Total Reward: -200000000\n",
      "Episode: 78, Total Reward: -200000000\n",
      "Episode: 79, Total Reward: -200000000\n",
      "Episode: 80, Total Reward: -200000000\n",
      "Episode: 81, Total Reward: -200000000\n",
      "Episode: 82, Total Reward: -200000000\n",
      "Episode: 83, Total Reward: -200000000\n",
      "Episode: 84, Total Reward: -200000000\n",
      "Episode: 85, Total Reward: -200000000\n",
      "Episode: 86, Total Reward: -200000000\n",
      "Episode: 87, Total Reward: -200000000\n",
      "Episode: 88, Total Reward: -200000000\n",
      "Episode: 89, Total Reward: -200000000\n",
      "Episode: 90, Total Reward: -200000000\n",
      "Episode: 91, Total Reward: -200000000\n",
      "Episode: 92, Total Reward: -200000000\n",
      "Episode: 93, Total Reward: -200000000\n",
      "Episode: 94, Total Reward: -200000000\n",
      "Episode: 95, Total Reward: -200000000\n",
      "Episode: 96, Total Reward: -200000000\n",
      "Episode: 97, Total Reward: -200000000\n",
      "Episode: 98, Total Reward: -200000000\n",
      "Episode: 99, Total Reward: -200000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "#dt 用于求微分，类似于单片机上的处理效果，假设是10k\n",
    "dt = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SMCenv:\n",
    "    def __init__(self):\n",
    "        # Define initial conditions and constants\n",
    "        # These values will need to be updated according to the specific scenario you are simulating\n",
    "        self.x1 = 0  # qβ initial condition\n",
    "        self.x2 = 0  # q˙β initial condition\n",
    "        self.am = 0  # nz*c*g, initial lateral overload\n",
    "        self.at = 0  # Target acceleration\n",
    "        self.R = 1000  # Initial relative distance, for example\n",
    "        self.Va = 306  # Speed of the aircraft\n",
    "        self.Vt = 310  # Speed of the target\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to the initial conditions\n",
    "        self.x1 = 0  # Reset qβ\n",
    "        self.x2 = 0  # Reset q˙β\n",
    "        self.am = 0  # Reset lateral overload\n",
    "        self.R = 1000  # Reset relative distance\n",
    "        # Return the initial state as a numpy array\n",
    "        return np.array([self.x1, self.x2])\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update the state based on the action\n",
    "        # action is expected to be the array [c, k, epsilon]\n",
    "        c, k, epsilon = action\n",
    "        \n",
    "        # Compute the state derivatives\n",
    "        x1_dot = self.x2\n",
    "        x2_dot = -(2 * self.R_dot / self.R) * self.x2 - (self.am / self.R) + (self.at / self.R)\n",
    "\n",
    "        # Update the state\n",
    "        self.x1 += x1_dot\n",
    "        self.x2 += x2_dot\n",
    "\n",
    "        # Compute the sliding surface\n",
    "        S = c * self.x2\n",
    "        S_dot = -k * S - epsilon * np.sign(S)\n",
    "\n",
    "        # Compute the lateral overload\n",
    "        U = -self.R / self.gravity * (-k * self.x2 - epsilon * np.sign(c * self.x2) - c * (-2 * self.R_dot / self.R * self.x2 - self.gravity / self.R + self.at / self.R))\n",
    "\n",
    "        # Compute reward, next_state, done, and any additional info\n",
    "        reward = -self.R**2\n",
    "        next_state = np.array([self.x1, self.x2])\n",
    "        done = False  # Define your own condition for 'done'\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    @property\n",
    "    def R_dot(self):\n",
    "        # Define how R_dot is computed based on the current state\n",
    "        # This is a placeholder for the actual computation\n",
    "        return -self.Vt * np.sin(self.x1) + self.Va * np.sin(self.x1)\n",
    "\n",
    "    @property\n",
    "    def gravity(self):\n",
    "        # Define the gravitational constant\n",
    "        return 9.81\n",
    "\n",
    "# Usage\n",
    "env = SMCenv()\n",
    "state = env.reset()\n",
    "for _ in range(1000):  # Example number of steps\n",
    "    action = np.random.random(3)  # Example action, replace with actual control algorithm\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Create DDPG network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        # 定义Actor网络结构\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # 定义如何从状态生成动作\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action = 5*torch.tanh(self.fc3(x)) #根据SMC参数进行调整\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # 定义Critic网络结构\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 定义如何评估状态和动作\n",
    "        x = torch.relu(self.fc1(torch.cat([state, action], 1)))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "#initialize Actor and critic\n",
    "state_dim = 2 # 环境状态维度 -e**2\n",
    "action_dim = 3 # 动作维度（SMC参数数量） c,k,epsi\n",
    "\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=5e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=5e-4)\n",
    "\n",
    "#training circle\n",
    "num_episodes = 100\n",
    "max_steps = 200  # 每个episode的最大步数\n",
    "batch_size = 128  # 从经验回放中采样的批量大小\n",
    "gamma = 0.6  #damping coefficient 0-1之前使时间越晚，奖励越少\n",
    "total_step = 0\n",
    "\n",
    "# 经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 初始化经验回放缓冲区\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "# 定义Transition具名元组，它有五个字段\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "#创建高斯噪声\n",
    "class GaussianNoise:\n",
    "    def __init__(self, action_dimension, mu=0, sigma=0.1, sigma_min=0.01, decay_rate=0.995):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.sigma_min = sigma_min\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def reset(self):\n",
    "        self.sigma = self.sigma * self.decay_rate\n",
    "        self.sigma = max(self.sigma, self.sigma_min)\n",
    "\n",
    "    def get_noise(self):\n",
    "        return np.random.normal(self.mu, self.sigma, self.action_dimension)\n",
    "\n",
    "#初始化噪声\n",
    "noise = GaussianNoise(action_dim)\n",
    "\n",
    "#training circle    \n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    total_step = total_step + 1\n",
    "    \n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # 转换state为PyTorch张量\n",
    "        # state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "        \n",
    "        # 生成动作\n",
    "        action = actor(state_tensor).detach().numpy().squeeze()\n",
    "        noise_sample = noise.get_noise()\n",
    "        action = action+noise_sample\n",
    "        \n",
    "        # 与环境交互\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # 存储经验\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # 准备下一个状态\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 如果经验回放缓冲区足够大，开始学习\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            # 从缓冲区采样一个批量的经验\n",
    "            transitions = replay_buffer.sample(batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # 提取变量\n",
    "            states = torch.tensor(np.array(batch.state), dtype=torch.float32)\n",
    "            actions = torch.tensor(np.array(batch.action), dtype=torch.float32)\n",
    "            rewards = torch.tensor(np.array(batch.reward), dtype=torch.float32)\n",
    "            next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32)\n",
    "            dones = torch.tensor(np.array(batch.done), dtype=torch.float32)\n",
    "\n",
    "            # 更新Critic\n",
    "            target_actions = actor(next_states)\n",
    "            target_q = critic(next_states, target_actions)\n",
    "            expected_q = rewards + (gamma * target_q * (1 - dones))\n",
    "            current_q = critic(states, actions)\n",
    "            critic_loss = F.mse_loss(current_q, expected_q.detach())\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # 更新Actor\n",
    "            predicted_actions = actor(states)\n",
    "            actor_loss = -critic(states, predicted_actions).mean()\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode: {episode}, Total Reward: {episode_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GJM23_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
